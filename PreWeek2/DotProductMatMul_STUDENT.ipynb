{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot Products and Matrix Multiplications\n",
    "> CogWorks 2019 (Petar Griggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Vector?\n",
    "\n",
    "We can represent a vector numerically as either a column or row of numbers:\n",
    "\\begin{equation}\n",
    "\\vec{u}=\\begin{bmatrix}3 \\\\ 4\\end{bmatrix}\\;\\;\\text{or}\\;\\;\\vec{u}=\\begin{bmatrix}3 & 4\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Visually, we can represent a vector as an arrow in space, connecting the origin to the point described by the vector:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{u}=\\begin{bmatrix}x \\\\ y\\end{bmatrix}\\;\\;\\text{or}\\;\\;\\vec{u}=\\begin{bmatrix}x & y\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "So, the vector $\\vec{u}=\\begin{bmatrix}3 \\\\ 4\\end{bmatrix}$ can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vec](pics/vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dimensionality* of a vector is really just the number of elements in that vector. On a 2D plane, we need two specify two values, $x$ and $y$, to localize ourselves; this is what is depicted above. In 3D space, we need three values, $x$, $y$, and $z$, to specify our location in space. And, in general, if we are in $N$-dimensions, we need to specify $N$ values to specify a point in that space. This idea generalizes to higher dimensions that we mere mortals can't visualize so easily.\n",
    "\n",
    "We can use a NumPy array to represent a vector in code. For example,\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{u}=\\begin{bmatrix}3 & 4\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "can simply be represented as\n",
    "\n",
    "```python\n",
    "np.array([3, 4])  # 1D array of size 2 represents a 2D vector\n",
    "```\n",
    "\n",
    "An important note on terminology: The \"dimensionality\" of the vector is **not** the same as the \"dimensionality\" of the array.\n",
    "\n",
    "The arrays:\n",
    "\n",
    "```python\n",
    "np.array([1.2])   # 1D array of size 1 represents a 1D vector\n",
    "np.array([3, 4])  # 1D array of size 2 represents a 2D vector\n",
    "np.array([-9.1, 4.0, 5.0])  # 1D array of size 3 represents a 3D vector\n",
    "```\n",
    "\n",
    "Are all 1-dimensional NumPy arrays, as they each only require 1 integer-index to uniquely specify an element in the array, but they represent a 1D, 2D, and 3D vector, respectively.\n",
    "\n",
    "Thus, all NumPy arrays with a single row (or column) of numbers are considered 1-dimensional arrays, in NumPy-lingo. The *size* (i.e. the number of elements in that array) of that array corresponds the dimensionality of the vector.\n",
    "\n",
    "As we will see below, a 2-dimensional NumPy array can be thought of as a matrix, or as an object that stores multiple vectors. E.g. the matrix\n",
    "\n",
    "\\begin{equation}\n",
    "X =\\begin{bmatrix}3 & 4 & 8 \\\\ 2 & 4 & 0\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "can be thought of as storing two 3-dimensional vectors, $\\begin{bmatrix}3 & 4 & 8\\end{bmatrix}$ and $\\begin{bmatrix}2 & 4 & 0\\end{bmatrix}$. \n",
    "\n",
    "We can represent this as a shape-(2, 3) NumPy array:\n",
    "\n",
    "```python\n",
    "x = np.array([[3, 4, 8], \n",
    "              [2, 4, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dot Product\n",
    "\n",
    "We can define the dot product operation to compute the similarity between two vectors. To start, we can compute the dot product as:\n",
    "\\begin{equation}\n",
    "\\vec{x}\\cdot\\vec{y}=\\sum_{i=0}^{n-1} x_i y_i\n",
    "\\end{equation}\n",
    "\n",
    "All this notation means is that we find the element-wise product of the two vectors (by multiplying the corresponding elements in each vector together), then sum the resultant vector. As an example, take vectors $\\vec{x}=\\begin{bmatrix}2 & 9\\end{bmatrix}$ and $\\vec{y}=\\begin{bmatrix}7 & 4\\end{bmatrix}$. Then we can compute the dot product as:\n",
    "\\begin{equation}\n",
    "\\vec{x}\\cdot\\vec{y}=(2 \\times 7) + (9 \\times 4) = 14 + 36 = 50\n",
    "\\end{equation}\n",
    "\n",
    "When we compute the dot product, we need to ensure that our vectors are of equal dimensionality. After all, if one vector has more elements than the other, how are we supposed to compute an element-wise product! It is also good to note that the result of a dot product will only be a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy has a handy [`matmul` function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html) that will compute the dot product between two vectors for us! \n",
    "\n",
    "```python\n",
    ">>> import numpy as np\n",
    "\n",
    ">>> x = np.array([2, 9])\n",
    ">>> y = np.array([7, 4])\n",
    ">>> np.matmul(x, y)\n",
    "50\n",
    "```\n",
    "\n",
    "The `@` operator is reserved by NumPy to invoke `np.matmul` - this is quite convenient:\n",
    "\n",
    "```python\n",
    ">>> x @ y  # the same as `np.matmul(x, y)`\n",
    "50\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned before that the dot product will help us measure the how much two vectors overlap with each other, or how similar they are. Geometrically-speaking, the dot product measures the acute angle, $\\theta$, between vectors $\\vec{x}$ and $\\vec{y}$ :\n",
    "\\begin{equation}\n",
    "\\cos(\\theta)=\\frac{\\vec{x}\\cdot\\vec{y}}{||\\vec{x}||||\\vec{y}||}\n",
    "\\end{equation}\n",
    "\n",
    "where $||\\vec{x}||=\\sqrt{x_0^2 + x_1^2 + ...}$ is the magnitude of a vector. If our two vectors are already normalized, such that $||\\vec{x}||=1$, then the dot product itself is $\\cos(\\theta)$.\n",
    "\n",
    "What good is this formula though? Well, it tells us that the angle between two vectors is proportional to the dot product. In that way, the dot product can tell us how much the to vectors overlap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few extreme cases to build an intuition. First, when two vectors are *parallel* and thus overlap completely, the angle between them will be $0$. As a simple example, conside the vectors illustrated below:\n",
    "\n",
    "![vec](pics/vec1.png)\n",
    "\n",
    "![vec](pics/vec2.png)\n",
    "\n",
    "---\n",
    "\n",
    "When two vectors are parallel, their dot product is simply the product of their maginitudes. In the case that both vectors are normalized, this value will be $1$. $\\theta=0$ also happens to be when cosine achieves its maximum. So, when two vectors are parallel, their dot product will be maximized.\n",
    "\n",
    "Below create two parallel vectors in NumPy and use `matmul` to find the dot product of the two vectors. Then, divide the dot product by the magnitudes of your vectors, using `np.linalg.norm` to find the magnitudes, to confirm that indeed $\\cos(\\theta)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # vector\n",
    "y = # vector\n",
    "\n",
    "dot_prod = # use np.matmul!\n",
    "\n",
    "cos_theta = dot_prod / # divide by the magnitude of your vectors\n",
    "\n",
    "print(cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two vectors are *antiparallel*, they face in opposite directions:\n",
    "\n",
    "![vec](pics/vec3.png)\n",
    "\n",
    "![vec](pics/vec4.png)\n",
    "\n",
    "---\n",
    "\n",
    "In this case, the angle between the two vectors is $180^\\circ$. However, when $\\theta=180$, cosine is at its minimum value of $-1$. Thus, when two vectors are antiparallel, their dot product will be minimized (and when two normalized vectors are antiparallel, their dot product will be $-1$).\n",
    "\n",
    "Just as before, create two antiparallel vectors in NumPy and use `matmul` to find the dot product of the two vectors. Divide the dot product by the magnitudes of your vectors to confirm that $\\cos(\\theta)=-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # vector\n",
    "y = # vector\n",
    "\n",
    "dot_prod = # use np.matmul!\n",
    "\n",
    "cos_theta = dot_prod / # divide by the magnitude of your vectors\n",
    "\n",
    "print(cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, consider two perpendicular vectors:\n",
    "\n",
    "![vec](pics/vec5.png)\n",
    "\n",
    "![vec](pics/vec6.png)\n",
    "\n",
    "---\n",
    "\n",
    "When two vectors are perpendicular to one another, they do not overlap at all. This also means that $\\theta=90^\\circ$, and consequently $\\cos(\\theta)=0$. Thus, the dot product of the two perpendicular vectors will always be $0$.\n",
    "\n",
    "Again, create two perpendicular vectors and use `matmul` to find the dot product. The result should confirm that for perpendicular vectors, $\\cos(\\theta)=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # vector\n",
    "y = # vector\n",
    "\n",
    "dot_prod = # use np.matmul!\n",
    "\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Matrices?\n",
    "\n",
    "Matrices have a number of interpretations, but we can simply consider a matrix to be a collection of vectors of the same dimensionality. If we have $M$ vectors, each of dimension $N$, we could then pack the vectors into a matrix. \n",
    "\n",
    "We can have each column of our matrix be a seperate vector:\n",
    "\\begin{align*}\n",
    "&\\quad\\;\\,\\begin{array}{ccc}\\leftarrow & M & \\rightarrow\\end{array} \\\\\n",
    "V = \\begin{array}{c}\\uparrow \\\\ N \\\\ \\downarrow\\end{array}\\;\\;&\\begin{bmatrix}\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\ \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_M \\\\ \\downarrow & \\downarrow & \\cdots & \\downarrow\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Here $V$ is a $(N,M)$ matrix. We could alternatively construct a matrix where each row is a distinct vector:\n",
    "\\begin{align*}\n",
    "&\\;\\;\\begin{array}{ccc}\\leftarrow & N & \\rightarrow\\end{array} \\\\\n",
    "W = \\begin{array}{c}\\uparrow \\\\ M \\\\ \\downarrow\\end{array}\\;\\;&\\begin{bmatrix}\\leftarrow & \\vec{w}_1 & \\rightarrow \\\\ \\leftarrow & \\vec{w}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{w}_M & \\rightarrow\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Here $W$ is a $(M,N)$ matrix. In either case, our matrix is simply a collection of vectors. In Python, we can represent a matrix as a 2D NumPy array (in fact, there is really no difference between a 2-dimensional NumPy array and a matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With matrices, we can define the matrix multiplication operation. Consider the two matrices $V$ and $W$, which have shapes $(M,N)$ and $(N,L)$, respectively. We will think of $V$ as having $M$ vectors of dimension $N$ (with each vector as a row) and $W$ as having $L$ vectors of dimension $N$ (with each vector as a column). Performing matrix multiplication will yield a $(M,L)$ matrix, where element $i,j$ is equal to:\n",
    "\\begin{equation}\n",
    "(VW)_{ij}=\\sum_{k=0}^{N-1}V_{ik}W_{kj}\n",
    "\\end{equation}\n",
    "\n",
    "This formula bears a striking resemblance to the dot product we defined earlier. In fact, by performing a matrix multiplication, we are actually performing repeated dot products. The dot product between the $i^\\text{th}$ row of $V$ and $j^\\text{th}$ column of $W$ is computed, then filled into element $i,j$ of our output matrix:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\begin{bmatrix}\\quad\\!\\uparrow & \\quad\\;\\;\\;\\;\\!\\uparrow & \\quad\\cdots & \\!\\uparrow \\\\ \\quad\\!\\vec{w}_1 & \\quad\\;\\;\\;\\vec{w}_2 & \\quad\\cdots & \\;\\;\\;\\;\\vec{w}_L\\;\\;\\;\\;\\:\\!\\!\\! \\\\ \\quad\\!\\downarrow & \\quad\\;\\;\\;\\;\\!\\downarrow & \\quad\\cdots & \\!\\downarrow\\end{bmatrix} \\\\\n",
    "VW=\\begin{bmatrix}\\leftarrow & \\vec{v}_1 & \\rightarrow \\\\ \\leftarrow & \\vec{v}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{v}_M & \\rightarrow\\end{bmatrix}&\\begin{bmatrix}\\vec{v}_1\\cdot\\vec{w}_1 & \\vec{v}_1\\cdot\\vec{w}_2 & \\cdots & \\vec{v}_1\\cdot\\vec{w}_L \\\\ \\vec{v}_2\\cdot\\vec{w}_1 & \\vec{v}_2\\cdot\\vec{w}_2 & \\cdots & \\vec{v}_2\\cdot\\vec{w}_L \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\vec{v}_M\\cdot\\vec{w}_1 & \\vec{v}_M\\cdot\\vec{w}_2 & \\cdots & \\vec{v}_M\\cdot\\vec{w}_L\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "It is important to note that the shapes of the matrices do matter. We can matrix multiply our $(M,N)$ and $(N,L)$ matrices ***only*** because the inner dimensions are both $N$. That is, the number of columns in matrix-1 and rows in matrix-2 are the same. We can not perform a matrix multiplication if this condition is not met.\n",
    "\n",
    "Because matrix multiplication is simply repeated dot products, each value we compute actually represents an angle between vectors. Thus, our out matrix when multiplying matrices $V$ and $W$ of shapes $(M,N)$ and $(N,L)$ is actually a matrix full of $M\\times L$ angles, where element $i,j$ tells us the similarity between column $i$ of matrix $W$ and row $j$ of matrix $V$.\n",
    "\n",
    "The [`matmul` function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html) that NumPy  provides also performs matrix multiplication on two arrays. Now, create and matrix multiply several arrays to get a feel for the function. Compare the result of the matrix multiplication with the dot product between a row and column in the two matrices to confirm that matrix multiplication is just a repeated dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
